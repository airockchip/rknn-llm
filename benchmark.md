# Model Performance Benchmark

- This performance data were collected based on the maximum CPU and NPU frequencies of each platform. 
- The script for setting the frequencies is located in the scripts directory.
- All models should be converted with `optimization_level` set to 0 to enable optimized runtime performance.

### RK3588

| Model     | Model Size | Dtype | Seqlen | New_tokens | TTFT(ms) | Tokens/s | memory(MB) |
| :-------- | :--------: | :---: | :----: | :--------: | :------: | :------: | :--------: |
| Qwen2     |    0.5B    | w8a8  |  128   |     64     |  372.54  |  40.26   |   639.72   |
| MiniCPM4  |    0.5B    | w8a8  |  128   |     64     |  156.01  |  43.93   |   507.45   |
| Qwen3     |    0.6B    | w8a8  |  128   |     64     |  215.76  |  29.52   |   772.98   |
| TinyLLAMA |    1.1B    | w8a8  |  128   |     64     |  294.6   |  24.26   |  1058.46   |
| Qwen2.5   |    1.5B    | w8a8  |  128   |     64     |  412.27  |  16.32   |  1659.15   |
| RWKV7     |    1.5B    | w8a8  |  128   |     64     |  828.62  |  13.81   |  1460.15   |
| InternLM2 |    1.8B    | w8a8  |  128   |     64     |  419.47  |   15.4   |  1764.16   |
| Gemma2    |     2B     | w8a8  |  128   |     64     |  693.95  |   9.65   |  2764.42   |
| TeleChat2 |     3B     | w8a8  |  128   |     64     |  657.6   |  10.12   |  2775.94   |
| Phi3      |    3.8B    | w8a8  |  128   |     64     | 1049.66  |   7.49   |  3747.69   |
| MiniCPM3  |     4B     | w8a8  |  128   |     64     |  1432.4  |   5.98   |  4337.68   |
| ChatGLM3  |     6B     | w8a8  |  128   |     64     |  1451.8  |   4.94   |  5883.87   |

### RK3576

| Model     | Model Size |   Dtype    | Seqlen | New_tokens | TTFT(ms) | Tokens/s | memory(MB) |
| :-------- | :--------: | :--------: | :----: | :--------: | :------: | :------: | :--------: |
| Qwen2     |    0.5B    |   w4a16    |  128   |     64     |  432.63  |  32.61   |   411.71   |
|           |    0.5B    | w4a16_g128 |  128   |     64     |  448.63  |  27.88   |   431.8    |
|           |    0.5B    |    w8a8    |  128   |     64     |  379.46  |  21.72   |   647.03   |
| MiniCPM4  |    0.5B    |   w4a16    |  128   |     64     |  414.05  |  36.76   |   305.61   |
|           |    0.5B    | w4a16_g128 |  128   |     64     |  420.83  |  33.75   |   346.41   |
|           |    0.5B    |    w8a8    |  128   |     64     |  377.83  |  23.94   |   511.98   |
| Qwen3     |    0.6B    |   w4a16    |  128   |     64     |  569.08  |  23.72   |   494.83   |
|           |    0.6B    | w4a16_g128 |  128   |     64     |   582    |  22.52   |   527.46   |
|           |    0.6B    |    w8a8    |  128   |     64     |   525    |  16.92   |   778.37   |
| TinyLLAMA |    1.1B    |   w4a16    |  128   |     64     |   706    |  21.02   |   573.3    |
|           |    1.1B    | w4a16_g128 |  128   |     64     |   822    |  18.91   |   655.82   |
|           |    1.1B    |    w8a8    |  128   |     64     |   619    |  12.59   |  1064.64   |
| Qwen2.5   |    1.5B    |   w4a16    |  128   |     64     |  959.23  |  14.45   |   932.98   |
|           |    1.5B    | w4a16_g128 |  128   |     64     | 1095.06  |  12.87   |  1015.79   |
|           |    1.5B    |    w8a8    |  128   |     64     |  814.69  |   8.51   |  1665.71   |
| RWKV7     |    1.5B    |   w4a16    |  128   |     64     | 2127.31  |  10.36   |   810.59   |
|           |    1.5B    | w4a16_g128 |  128   |     64     | 2229.17  |   9.65   |   901.84   |
|           |    1.5B    |    w8a8    |  128   |     64     | 1878.21  |   7.17   |  1469.38   |
| InternLM2 |    1.8B    |   w4a16    |  128   |     64     |  970.65  |  13.62   |   964.26   |
|           |    1.8B    | w4a16_g128 |  128   |     64     | 1150.83  |  12.06   |  1059.62   |
|           |    1.8B    |    w8a8    |  128   |     64     |  820.58  |   7.9    |  1771.28   |
| Gemma2    |     2B     |   w4a16    |  128   |     64     | 1262.21  |   8.49   |  1527.76   |
|           |     2B     | w4a16_g128 |  128   |     64     |   1535   |   7.72   |   1615.4   |
|           |     2B     |    w8a8    |  128   |     64     | 1126.28  |   4.92   |  2770.05   |
| TeleChat2 |     3B     |   w4a16    |  128   |     64     | 1356.45  |   8.98   |  1513.85   |
|           |     3B     | w4a16_g128 |  128   |     64     |  1585.8  |   7.82   |  1632.43   |
|           |     3B     |    w8a8    |  128   |     64     | 1129.12  |   5.13   |  2782.64   |
| Phi3      |    3.8B    |   w4a16    |  128   |     64     | 1980.87  |   6.35   |   1985.4   |
|           |    3.8B    | w4a16_g128 |  128   |     64     | 2392.96  |   5.84   |   2141.5   |
|           |    3.8B    |    w8a8    |  128   |     64     | 1641.84  |   3.75   |  3756.92   |
| MiniCPM3  |     4B     |   w4a16    |  128   |     64     | 2899.35  |   4.94   |  2334.24   |
|           |     4B     | w4a16_g128 |  128   |     64     | 3377.92  |   4.49   |  2615.88   |
|           |     4B     |    w8a8    |  128   |     64     | 2621.17  |   3.03   |  4364.42   |
| ChatGLM3  |     6B     |   w4a16    |  128   |     64     | 2362.78  |   4.62   |  2983.15   |
|           |     6B     | w4a16_g128 |  128   |     64     | 3170.31  |    4     |  3196.36   |
|           |     6B     |    w8a8    |  128   |     64     | 2037.96  |   2.29   |  5894.02   |

### RK3562

| Model    | Model Size | Dtype | Seqlen | New_tokens | TTFT(ms) | Tokens/s | memory(MB) |
| :------- | :--------: | :---: | :----: | :--------: | :------: | :------: | :--------: |
| Qwen2    |    0.5B    | w8a8  |  128   |     64     |  946.58  |  11.46   |   625.14   |
| MiniCPM4 |    0.5B    | w8a8  |  128   |     64     |  905.69  |  10.47   |   492.4    |
| Qwen3    |    0.6B    | w8a8  |  128   |     64     | 1248.58  |   8.87   |   755.48   |

### RV1126B

| Model    | Model Size |   Dtype    | Seqlen | New_tokens | TTFT(ms) | Tokens/s |
| :------- | :--------: | :--------: | :----: | :--------: | :------: | :------: |
| Qwen2    |    0.5B    |   w4a16    |  128   |     64     |  975.36  |  16.98   |
|          |    0.5B    | w4a16_g128 |  128   |     64     |  831.39  |   15.3   |
|          |    0.5B    |    w8a8    |  128   |     64     |  969.68  |   11.7   |
| MiniCPM4 |    0.6B    |   w4a16    |  128   |     64     |  941.48  |  20.37   |
|          |    0.6B    | w4a16_g128 |  128   |     64     |  862.57  |  17.73   |
|          |    0.6B    |    w8a8    |  128   |     64     |  955.15  |  13.47   |

### Multimodal

| model         |        Stage         |  RK3588(w8a8)  | RK3576(w4a16) |
| :------------ | :------------------: | :------------: | :-----------: |
| Qwen2-VL-2B   | img-encoder(392*392) |     3.28s      |     3.55s     |
|               |   Prefill(len=196)   |    693.4ms     |   1533.8ms    |
|               |        Decode        | 16.29 tokens/s | 14.1 tokens/s |
| Qwen2.5-VL-3B | img-encoder(392*392) |     2.93s      |     2.87s     |
|               |   Prefill(len=196)   |     1262ms     |    2656ms     |
|               |        Decode        | 8.66 tokens/s  | 7.82 tokens/s |
| MiniCPM-V-2_6 | img-encoder(448*448) |     3.27s      |     2.4s      |
|               |   Prefill(len=64)    |    869.5ms     |    1415ms     |
|               |        Decode        | 4.04 tokens/s  | 3.94 tokens/s |
| SmolVLM-256M  | Img-encoder(512*512) |     842ms      |     768ms     |
|               |   Prefill(len=128)   |     87.2ms     |     251ms     |
|               |        Decode        | 77.7 tokens/s  | 54.8 tokens/s |

- The img-encoder runs inference on RKNN with FP16, tested using all NPU cores.